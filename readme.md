ðŸ”— Course link-> [link](https://www.udemy.com/course/python-sdet-rest-api-automation/)

## ðŸ“Œ Sections
Below are the summary what i learnt from each section.
- Section 1
    - Installation of Python, PIP, PyCharm
    - What we will be learing through out the course
- Section 2
    - Data Types
    - Data Structure -> List, String, Tuple, Dictionary
    - Loops and condition statements
    - Functions
    - OOPS
    - Reading data from files

- Section 3-6
    - Project-1
    - Automating the manual Test cases
    - Project 1 --> Knowledge of Python Data Structure is heavily needed 
        - Json Parsers with Python Modules
        - Read and write to Json files with Python
        - API Automation using request Library
        - Understand CRUD operation(GET, POST, FELETE, PUT) API for automation
        - Parsing API response with Python utilities.

    
<!-- - Section 4
- Section 5
- Section 6 -->
- Section 7-8
    - [Project-2](./Projects/Project2/)

<!-- - Section 8 -->
- Section 9


- Section 10
    - Project-3
    - csv parsing 



- Section 11-12
    - Project-4
<!-- - Section 12 -->


- Section 13
    - Project-5
    - web scrapping
    - It is all about Extracting data/content from web pages
    - Python does this with the help of its available Beautiful Soup Package
    - Web Scrapping is highly used in Data Science to analyze and prepare metrics
    - "Find data and play with it" is one of the key principle in any saying Data Science Projects and python scrapping is the most common technique used to find data



- [Section 14](./Course%20notes/)
- Section 15




--------------------------------
Folders Information
-  [Course notes](./Course%20notes/) contains the notes from the udemy  
- Projects
    - 1. json parsing
        -  import json package 
        -  load is used for external file [json.load(file path in r/w mode)]
        -  loads is used for string--> [json.load(variable_data)]
    <br> <hr>
    - 2. Database interaction with Python SQL Connector
        - follow this [file](./Projects/Project2/SQL.txt)
    <br> <hr>
    - 3. CSV parsing
        - import csv
        - use csv.reader() for reading the csv file
        - used writer() for writing in the file
        - use writerow() for writing the rows
    <br> <hr>
    - 4. Establishing SSH connection to linux using python Paramiko
        - [ssh](./Projects/Project4/)
    <br> <hr>
    - 5. Web Scrapping
        - from bs4 import BeautifulSoup
        - import requests
        - check this [file](./Projects/Project5/webScrapping.py) as written there only

<!-- 
p1->
p2->
p3->
p4->
p5->
 -->